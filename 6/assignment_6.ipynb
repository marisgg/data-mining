{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Maris Galesloot\"\n",
    "STUDENT_NUMBER = \"s4634098\"\n",
    "COLLABORATOR_NAME = \"Kamiel Kunst\"\n",
    "COLLABORATOR_STUDENT_NUMBER = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a531b239c4400830d0b0b437834e0a0f",
     "grade": false,
     "grade_id": "cell-4543b21b34b8c4a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 6: Nearest Neighbour and Artificial Neural Networks\n",
    "\n",
    "## Objective of this assignment\n",
    "The objective of this exercise is to understand how k-nearest neighbor and neural networks can be used to solve classification problems.\n",
    "\n",
    "## ** Important: ** When handing in your homework:\n",
    "+ Hand in the notebook (and nothing else) named as follows: StudentName1_snumber_StudentName2_snumber.ipynb\n",
    "+ Provide clear and complete answers to the questions below under a separate header (not hidden somewhere in your source code), and make sure to explain your answers / motivate your choices. Add Markdown cells where necessary.\n",
    "+ Source code, output graphs, derivations, etc., should be included in the notebook.\n",
    "+ Hand-in: upload to Brightspace.\n",
    "+ Include name, student number, assignment (especially in filenames)!\n",
    "+ When working in pairs only one of you should upload the assignment, and report the name of your partner in your filename.\n",
    "+ For problems or questions: come to the practical sessions or email the student assistants.\n",
    "\n",
    "\n",
    "## Advised Reading and Exercise Material\n",
    "**The following reading material is recommended:**\n",
    "\n",
    "- Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, *Introduction to Data Mining*, section 5.2-5.4.\n",
    "\n",
    "\n",
    "## Additional Tools\n",
    "This exercise is based upon material kindly provided by the Cognitive System Section, DTU Compute, http://cogsys.compute.dtu.dk. Any sale or commercial distribution is strictly forbidden.\n",
    "\n",
    "\n",
    "##  6.1 K-nearest neighbor classification\n",
    "In this exercise we will use the k-nearest neighbors (KNN) method for classification.\n",
    "First, we will consider the four synthetic data sets synth1, synth2, synth3 and\n",
    "synth4 we used in earlier assignments.\n",
    "\n",
    "#### 6.1.1 (2 points)\n",
    "For each of the four synthetic data sets, do the following.:\n",
    "\n",
    "* Load the training part of the dataset `X_train` and `y_train` as well as `X_test` and `y_test`.\n",
    "\n",
    "* Fit a  k-nearest neighbor classifier model (`KNeighborsClassifier` from `sklearn.neighbors` (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) on the train data. \n",
    "  * Choose a suitable distance measure (you should consider the distance measures `euclidean`,`cityblock`, `cosine`, and `seuclidean`). \n",
    "\n",
    "  * Choose a suitable number of neighbors. \n",
    "  \n",
    "* Predict the class of the test data using the trained model.\n",
    "\n",
    "* Make a scatterplot of the train and test data with the classification of the test data obtained with the best k-value and distance measures you found -- just one plot per data set is fine. You can use the `classification_plot` function from the toolbox. \n",
    "  * Use the obtained prediction of the test data in your plot.\n",
    "  * Try to study the plot (use all 5 arguments) to see how test data is classified using the train data.\n",
    "\n",
    "* Create the confusion matrix, plot it using the `plot_confusion_matrix` function given below, and calculate the accuracy and error rate. Print or show the accuracy and error rate for each dataset.\n",
    "\n",
    "Answers the following questions for each dataset:\n",
    "\n",
    "* Which distance measures worked best for each of the four problems? Can you explain why?\n",
    "\n",
    "* How many neighbors were needed for the four problems? \n",
    "\n",
    "* Can you give an example of when it would be good to use a large/small number of neighbors? Consider e.g. when clusters are  well separated versus when they are overlapping.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* To generate a confusion matrix, you can use the function confusion_matrix() from the module sklearn.metrics. You can use the function below to plot the confusion matrix. Don't remember how to read a confusion matrix? Check the wiki page: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cedae35cd068fa593fe2e56341aa9a48",
     "grade": false,
     "grade_id": "cell-881a8d7329bd61d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm,y):\n",
    "    df_cm = pd.DataFrame(cm, index = [i+1 for i in np.unique(y)],\n",
    "                  columns = [i+1 for i in np.unique(y)])\n",
    "    plt.figure()\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('Actual class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26f2ff9aaa7e1840deeb38980d13ff47",
     "grade": true,
     "grade_id": "cell-c914718fc664cbf0",
     "locked": false,
     "points": 100,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.1.1\n",
    "# YOUR CODE HERE\n",
    "import scipy.io as scio\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from toolbox.classification_plot import classification_plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "for i in range(1,5):\n",
    "    data = scio.loadmat(\"data/synth\" + str(i) + \".mat\")\n",
    "    X_test = data['X_test']\n",
    "    X_train = data['X_train']\n",
    "    y_test = data['y_test'].ravel()\n",
    "    y_train = data['y_train'].ravel()\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    classification_plot(X_test, y_test, y_pred, X_train, y_train)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(cm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f119a5a62deb0a9bf9d45636a42bcb7",
     "grade": true,
     "grade_id": "cell-c365d26e6855010f",
     "locked": false,
     "points": 100,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59ac7546b659465ab4fa46efeca1a50f",
     "grade": false,
     "grade_id": "cell-8ecc7df13886d8fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.1.2 (1 points)\n",
    "In general we can use cross-validation to select the optimal distance metric and number of nearest neighbors, k.This can however be computationally expensive. \n",
    "\n",
    "We now return to the Iris data that we have considered in previous exercises, and will attempt to classify the Iris flowers using KNN. \n",
    "\n",
    "* Load the Iris data into Python with the `pandas` function `read_excel()` and save it to a variable called `iris_data`. Inspect the data by printing the `head()` of the dataframe. \n",
    "\n",
    "* Use the values of the 4 variables `Sepal Length,  Sepal Width,  Petal Length,  Petal Width` to create a data set `X`. Use the `Type` column to create the labels `labels`. Both `X` should be a numpy array! You can use the `to_numpy()` method for this. `labels` will be a Pandas series.\n",
    "\n",
    "* Convert the Pandas series `labels` to an integer encoding by using the `LabelEncoder` in scikit-learn. Save the integer encoding, which should be a numpy array to a variable called `y`.\n",
    "\n",
    "* Use leave-one-out cross-validation to estimate the optimal number of neighbors, k, for the k-nearest neighbor classifier. Save the vector of averaged errors to a variable called `mean_errors`. This should be a numpy array.\n",
    "\n",
    "* Plot the cross-validation average classification error as a function of k for $k = 1,..,40$.\n",
    "\n",
    "You can use the function `LeaveOneOut` from `sklearn.model_selection`: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html\n",
    "\n",
    "What is the optimal number of neighbors to use for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bce13c66bbd7e3011f794f5ec64f5b4",
     "grade": false,
     "grade_id": "cell-d4a09338afe0c7e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.1.2\n",
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "le = preprocessing.LabelEncoder()\n",
    "iris_data = pd.read_excel(\"data/iris.xls\")\n",
    "X = iris_data[[\"Sepal Length\", 'Sepal Width', 'Petal Length', 'Petal Width']].to_numpy()\n",
    "labels = iris_data[\"Type\"]\n",
    "le.fit(labels)\n",
    "y = le.transform(labels)\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "counter = 1\n",
    "mean_errors = []\n",
    "for k in range(1,40):\n",
    "    errors = []\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        y_pred = neigh.predict(X_test)\n",
    "        test_error = 1 - accuracy_score(y_test, y_pred, normalize=True)\n",
    "        errors.append(test_error)\n",
    "    mean_errors.append(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot = plt.figure()\n",
    "print(\"Figure 1:\")\n",
    "plt.plot(range(1,40), mean_errors, color='red', label='Error')\n",
    "plt.ylabel('Error')\n",
    "# Set the y axis label of the current axis.\n",
    "plt.xlabel('Nearest Neighbours (k)')\n",
    "# Set a title of the current axes.\n",
    "plt.title('Mean error using LOO cross-validation of k in nearest neighbours.')\n",
    "# plt.xticks(np.arange(min(range(1,40)), max(range(1,40))+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f88c28dac836b1e35ed742f320329f1",
     "grade": true,
     "grade_id": "cell-5228b8b55ea59d61",
     "locked": true,
     "points": 80,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Checks whether 6.1.2 output is correct\"\"\"\n",
    "\"\"\"DO NOT MODIFY THIS CELL\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8281ec59a17b23d98db14a22fe213760",
     "grade": true,
     "grade_id": "cell-28bebc3e081a2ae6",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "From the plot of Figure 1, k = 20 is evidently the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82a242682254f8612636192a23e60623",
     "grade": false,
     "grade_id": "cell-58214337905e988d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.1.3 (2 points)\n",
    "\n",
    "KNN can also be used for regression by predicting the output of an observation as the average of the output values of its nearest neighbors. \n",
    "\n",
    "KNN regression in Python is most easily performed using the `KNeighborsRegressor` in `sklearn.neighbors`. \n",
    "\n",
    "We'll predict the alcohol content of wine in the wine data with KNN, using the other 10 attributes as predictors. \n",
    "\n",
    "* Load the Wine data into Python and save it to a variable called `wine_data`.\n",
    "\n",
    "* Construct the `X` and `y` numpy arrays by slicing the `wine_data`. `y` should be a vector containing only the alcohol percentage, the other variables should be contained in `X`.\n",
    "\n",
    "* We will use the mean squared error as a performance metric. This can be calculated using `mean_squared_error` in `sklearn.metrics`.\n",
    "\n",
    "* Use k-fold cross-validation with k=5 to estimate the optimal number of neighbors, k, for the k-nearest neighbor classifier. Save the vector of averaged errors to a variable called `mean_errors`. This should be a numpy array. **N.B.** This means you will have a mean of mean squared errors for each value of k.\n",
    "\n",
    "* The cross-validation procedure should be performed using `KFold` in `sklearn.model_selection`. The variable `random_state` **must** be set to a value of `42`. The variable `shuffle` must be set to `True`.\n",
    "\n",
    "* Plot the calculated cross-validation averaged mean square error as a function of k for $k = 1,..,40$.\n",
    "\n",
    "* Repeat all these steps, but perform standardization during the procedure. Make sure the mean and standard deviation are calculated over **only** the train data, and applied on both train and the test data. You can use the `StandardScaler` in `sklearn.preprocessing` for this. Save the errors to `mean_errors_std` using the same methodology as for `mean_errors`.\n",
    "\n",
    "Then answer the following questions:\n",
    "* What is the optimal value for the number of nearest neighbors? Answer for both the preprocessing procedures (no preprocessing and standardization).\n",
    "\n",
    "* Does the algorithm perform better with or without standardization? Explain your observations.\n",
    "\n",
    "* Why do we need to calculate the mean and standard deviation on the training data rather than over the entire data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1a4b383943d5c7a41600b4a3b517eb8",
     "grade": false,
     "grade_id": "cell-504d617605676706",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.1.3\n",
    "# YOUR CODE HERE\n",
    "wine_data = scio.loadmat(\"data/wine.mat\")\n",
    "X = wine_data['X']\n",
    "y = wine_data['y'].ravel()\n",
    "attribute_names = [x[0] for x in wine_data['attributeNames'].ravel()]\n",
    "class_names = [x[0] for x in wine_data['classNames'].ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "822e7292a5a51db93c5e9665488c136e",
     "grade": true,
     "grade_id": "cell-4b08ebfcacac67d5",
     "locked": true,
     "points": 150,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Checks whether 6.1.2 output is correct\"\"\"\n",
    "\"\"\"DO NOT MODIFY THIS CELL\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e7ed8399a8da8c0b78516cf4f71e3aa",
     "grade": true,
     "grade_id": "cell-4100da3d38a4de04",
     "locked": false,
     "points": 50,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ace40b8ed6b0fae9f27a48ad47c0677",
     "grade": false,
     "grade_id": "cell-d5c684910893698d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6.2 Artificial Neural Networks\n",
    "\n",
    "In this part of the exercise we will use neural networks to classify the xor data. We will consider a network with an input layer, one layer of hidden units and an output layer. The class `sklearn.neural_network.MLPClassifier` can be used to create a Multilayer Perceptron that can minimizes the Cross-Entropy loss function for any dataset X with corresponding labels y. See https://scikit-learn.org/stable/modules/neural_networks_supervised.html \n",
    "\n",
    "We use the data contained in `xor.mat` in the Data folder. \n",
    "\n",
    "#### 6.2.1 (0.5 points)\n",
    "Check out the documentation for `MLPClassifier` and read the documentation well. Make sure you understand at least in general terms how the learning process works. Answer the following questions before you continue:\n",
    "\n",
    "1. For a single perceptron, the activation function can be linear, e.g. $f(x) = x$. However, the activation function used in the MLP class is a non-linear function. Why does it not make sense for a MLP to use such a linear activation function? Hint: what does the following computation simplify to if $f(x)$ is such a simple linear function: $f(Wo*f(Wh*X_i))$?\n",
    "2. The MLPClassifier has a few optional parameters. For each of the following parameters, explain how changing the parameter might affect the learning process or the resulting solution:\n",
    "\n",
    "> `hidden_layer_sizes`:\n",
    "\n",
    "> `max_iter`:\n",
    "\n",
    "> `learning_rate`:\n",
    "\n",
    "> `learning_rate_init`:\n",
    "\n",
    "   3\\. Use the following commands to create a small test set:\n",
    "\n",
    "> `Xtest = np.array([[0,0],[0,1],[1,0],[1,1]]).`\n",
    "\n",
    "> `ytest = np.array([0,0,0,1])`\n",
    "     \n",
    "\n",
    "Create a MLPClassifier with 1 hidden layer using the `lbfgs`solver and fit the data. You can leave the other parameters unchanged. Use the score method to compute the mean accuracy. How well does the MLP perform on this problem? Use the function `MLPPlot.plot_boundaries` function to plot the data and the decision boundaries. Why does(n't) it work well with one hidden unit? Could you improve by using more?\n",
    "\n",
    "#### NB: the weights are initialised randomly, so you should run the code a few times untill you get a decent result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bfe7e2649db80247ed098f87b38a1e",
     "grade": true,
     "grade_id": "cell-735ec50c840768e5",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.1\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d2db60b6ac94b26be0ce4f22c3792ef",
     "grade": true,
     "grade_id": "cell-9d41b67efb609fc1",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "504a4551a7e5b45b243e8b8176a03326",
     "grade": false,
     "grade_id": "cell-c65eb2c09f090f0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.2.2 (0.5 points)\n",
    "Load the data from `xor.mat` into Python using `scipio.io.loadmat(...)` and make `y` a 1d vector. Make a scatter plot of the two attributes in `X`, coloring the points according to the class labels `y` and add a legend. Use matplotlib functions, and no toolbox functions for this. How are `X` and `y` related?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14c2c43389f769b8e06271ea4e60cce4",
     "grade": true,
     "grade_id": "cell-29ae1f3886162c7e",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.2\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd494932ccb21ee27e9cd329566ae560",
     "grade": true,
     "grade_id": "cell-32ed8b71c9f4daa2",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e524801fc35bb94de10b197f23e0ced5",
     "grade": false,
     "grade_id": "cell-651d2f68691aeb95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.2.3 (2 points)\n",
    "Create a MLPClassifier with 1 hidden unit. Fit X and y. Use 10-fold cross-validation `KFold` from `sklearn.model_selection` to create training and test sets and estimate the classification error for both. In each fold, run the learning process 5 times and take the best classification error.  \n",
    "\n",
    "Plot the decision boundaries of one network trained on the entire data set, again with 1 hidden unit, and explain why the network performs so well/poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1600309d01b54329c3addff47903301",
     "grade": true,
     "grade_id": "cell-43a21e78b44b6141",
     "locked": false,
     "points": 100,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.3\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7289e611666fcc24ee8c59072a061f8",
     "grade": true,
     "grade_id": "cell-adb15a9e87187331",
     "locked": false,
     "points": 100,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "343626467d3adc9bcb1f9e9c3da2927d",
     "grade": false,
     "grade_id": "cell-1936656c8e08a612",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.2.4 (1 point)\n",
    "Repeat 6.2.3, but use two hidden units instead of one. Does the classification performance improve? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e82241fbd0bc6a0965b36a79d5264fda",
     "grade": true,
     "grade_id": "cell-a39abd58bfcca041",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.4\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "17e052b110aefbd9b76363a6ca86f274",
     "grade": true,
     "grade_id": "cell-dd67a9822341f970",
     "locked": false,
     "points": 100,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d1a3f961df0f6044eb3e77f6e388dfb",
     "grade": false,
     "grade_id": "cell-981cc4f0a8d12be4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 6.2.5 (1 point)\n",
    "Repeat 6.2.3 with 10 hidden units. What happens to the decision boundaries of the learned neural networks? What are the benefits and drawbacks of including\n",
    "many hidden units in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b61ecd4dd4fddce66357c1af9618b19",
     "grade": true,
     "grade_id": "cell-7f016c36f90eef22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##Answer to question 6.2.5\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cef002189b504ffe1bdec1d08086cf6",
     "grade": true,
     "grade_id": "cell-30e1c8dd5d124df0",
     "locked": false,
     "points": 100,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
